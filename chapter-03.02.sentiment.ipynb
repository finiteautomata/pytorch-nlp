{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Sentiment of Restaurant Reviews\n",
    "\n",
    "## The Yelp Review Dataset\n",
    "In 2015, Yelp held a contest in which it asked participants to predict the rating of a restaurant given its review. Zhang, Zhao, and Lecun (2015) simplified the dataset by converting the 1- and 2-star ratings into a “negative” sentiment class and the 3- and 4-star ratings into a “positive” sentiment class, and split it into 560,000 training samples and 38,000 testing samples. In this example we use the simplified Yelp dataset, with two minor differences. In the remainder of this section, we describe the process by which we minimally clean the data and derive our final dataset. Then, we outline the implementation that utilizes PyTorch’s Dataset class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the dataset. Instead of just using `pandas` or whatsoever, we load it with `ReviewDataset`, which basically wraps the `pandas` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Length of dataset: 39200\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from text import ReviewDataset, ReviewVectorizer, Vocabulary\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "dataset = ReviewDataset.load_dataset_and_make_vectorizer(\n",
    "    'data/yelp/reviews_with_splits_lite.csv',\n",
    ")\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "print(\"Length of dataset: {}\".format(len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ReviewDataset` inherits from Dataset, an abstract class which defines some methods to be defined by their concrete descendants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"An abstract class representing a Dataset.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    All other datasets should subclass it. All subclasses should override\u001b[0m\n",
       "\u001b[0;34m    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\u001b[0m\n",
       "\u001b[0;34m    supporting integer indexing in range from 0 to len(self) exclusive.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mConcatDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/torch/utils/data/dataset.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     TensorDataset, ConcatDataset, Subset, DataframeDataset\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Dataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_data': array([1., 1., 1., ..., 0., 0., 0.], dtype=float32), 'y_target': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vectorize(\"hello I love this\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a very basic perceptron classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ReviewClassifier(nn.Module):\n",
    "    \"\"\" a simple perceptron-based classifier \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): the size of the input feature vector\n",
    "        \"\"\"\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, \n",
    "                             out_features=1)\n",
    "\n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor \n",
    "                x_in.shape should be (batch, num_features)\n",
    "            apply_sigmoid (bool): a flag for the sigmoid activation\n",
    "                should be false if used with the cross-entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch,).\n",
    "        \"\"\"\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = F.sigmoid(y_out)\n",
    "        return y_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and Optimizer\n",
    "\n",
    "Here, we define the loss function and the optimizer\n",
    "\n",
    "`BCEWithLogitsLoss` is the Binary Cross-Entropy function. In Pytorch, there are two versions: `BCELoss` and `BCEWithLogitsLoss`. The difference between them is that while BCELoss expects the output of a sigmoid function, the latter expects the logits. If you go into the maths, you will find more numerically stable to derive directly the Cross-Entropy of the logits.\n",
    "\n",
    "Regarding the optimizer, we will use `Adam` –a fairly standard option nowadays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "# loss and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now, let's train our model.\n",
    "\n",
    "As our model returns the logits, we have to apply sigmoid first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()\n",
    "    \n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training will basically consist of *epochs*, which are complete passes through all the elements of our datasets. \n",
    "\n",
    "In Stochastic Gradient Descent, we don't use all the dataset at each time but instead calculate the gradient from smaller `batches`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_data', 'y_target'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader(dataset=dataset, batch_size=1000)\n",
    "\n",
    "batch_iter = iter(loader)\n",
    "\n",
    "batch = next(batch_iter)\n",
    "\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train our model\n",
    "\n",
    "**Obs**: there's a tricky calculation on `running_loss` and `running_acc`. It is basically an average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5   epoch\n",
      "10  epoch\n",
      "15  epoch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 128\n",
    "# Let's save the info here\n",
    "\n",
    "train_state ={\n",
    "    'epoch_index': 0,\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'test_loss': -1,\n",
    "    'test_acc': -1\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for epoch_index in range(epochs):\n",
    "    if epoch_index % 5 == 0 and epoch_index > 0:\n",
    "        print(\"{:<3} epoch\".format(epoch_index))\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "\n",
    "    # Iterate over training dataset\n",
    "\n",
    "    # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "    dataset.set_split('train')\n",
    "    \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                        shuffle=True, drop_last=True)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(dataloader):\n",
    "        # the training routine is 5 steps:\n",
    "\n",
    "        # step 1. zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # step 2. compute the output\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        \n",
    "        # This is \n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # step 4. use loss to produce gradients\n",
    "        loss.backward()\n",
    "        # step 5. use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # compute the accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "    # Iterate over val dataset\n",
    "\n",
    "    # setup: batch generator, set loss and acc to 0, set eval mode on\n",
    "    dataset.set_split('val')\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                        shuffle=True, drop_last=True)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(dataloader):\n",
    "\n",
    "        # step 1. compute the output\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "        # step 2. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # step 3. compute the accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing in held-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                    shuffle=True, drop_last=True)\n",
    "\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(dataloader):\n",
    "    # compute the output\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.214\n",
      "Test Accuracy: 91.85\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Model Weights\n",
    "\n",
    "Which are the most predictive features for positive reviews? Let's look for those with highest positive weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential words in Positive Reviews:\n",
      "--------------------------------------\n",
      "delicious (1.611)\n",
      "fantastic (1.460)\n",
      "pleasantly (1.413)\n",
      "amazing (1.401)\n",
      "great (1.304)\n",
      "vegas (1.291)\n",
      "excellent (1.249)\n",
      "yum (1.241)\n",
      "ngreat (1.220)\n",
      "perfect (1.217)\n",
      "awesome (1.212)\n",
      "yummy (1.186)\n",
      "love (1.143)\n",
      "bomb (1.117)\n",
      "solid (1.083)\n",
      "wonderful (1.045)\n",
      "pleased (1.044)\n",
      "notch (1.039)\n",
      "chinatown (1.025)\n",
      "perfection (1.011)\n"
     ]
    }
   ],
   "source": [
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "sorted_weights, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
    "indices = indices.numpy().tolist()\n",
    "\n",
    "# Top 20 words\n",
    "print(\"Influential words in Positive Reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(20):\n",
    "    word = vectorizer.review_vocab.lookup_index(indices[i])\n",
    "    weight = sorted_weights[i]\n",
    "    print(\"{} ({:.3f})\".format(word, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential words in Negative Reviews:\n",
      "--------------------------------------\n",
      "worst (0.479)\n",
      "mediocre (0.611)\n",
      "bland (0.341)\n",
      "horrible (0.491)\n",
      "meh (0.288)\n",
      "awful (0.516)\n",
      "rude (0.542)\n",
      "terrible (1.460)\n",
      "tasteless (0.389)\n",
      "overpriced (0.531)\n",
      "disgusting (0.478)\n",
      "unacceptable (0.144)\n",
      "slowest (-0.127)\n",
      "poorly (0.182)\n",
      "unfriendly (0.392)\n",
      "nmaybe (0.018)\n",
      "disappointing (0.335)\n",
      "disappointment (0.104)\n",
      "inconsistent (-0.013)\n",
      "underwhelmed (0.072)\n"
     ]
    }
   ],
   "source": [
    "# Top 20 negative words\n",
    "print(\"Influential words in Negative Reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "indices.reverse()\n",
    "for idx in indices[:20]:\n",
    "    word = vectorizer.review_vocab.lookup_index(idx)\n",
    "    weight = sorted_weights[idx]\n",
    "    print(\"{} ({:.3f})\".format(word, weight))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
