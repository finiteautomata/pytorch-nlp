{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick tour to NLP\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "(with nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':-)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet=u\"Snow White and the Seven Degrees #MakeAMovieCold@midnight:-)\"\n",
    "tokenizer = TweetTokenizer()\n",
    "print(tokenizer.tokenize(tweet.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', ',', 'do', 'n’t', 'slap', 'the', 'green', 'witch']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "text = \"Mary, don’t slap the green witch\"\n",
    "print([str(token) for token in nlp(text.lower())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmas and Stems\n",
    "**Lemmas** are root forms of words. Consider the verb fly. It can be inflected into many different words—flow, flew, flies, flown, flowing, and so on—and fly is the lemma for all of these seemingly different words. Sometimes, it might be useful to reduce the tokens to their lemmas to keep the dimensionality of the vector representation low. This reduction is called lemmatization, and you can see it in action here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he --> -PRON-\n",
      "was --> be\n",
      "running --> run\n",
      "late --> late\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "doc = nlp(u\"he was running late\")\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, token.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Stemming** is the poor-man’s lemmatization.3 It involves the use of handcrafted rules to strip endings of words to reduce them to a common form called stems. Popular stemmers often implemented in open source packages include the Porter and Snowball stemmers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary - PROPN\n",
      "slapped - VERB\n",
      "the - DET\n",
      "green - ADJ\n",
      "witch - NOUN\n",
      ". - PUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u\"Mary slapped the green witch.\")\n",
    "\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qué - ADJ\n",
      "onda - NOUN\n",
      "amiguitoooooo - NOUN\n",
      ", - PUNCT\n",
      "cómo - VERB\n",
      "está - ADJ\n",
      "todo - NOUN\n",
      "en - X\n",
      "ese - ADJ\n",
      "bello - NOUN\n",
      "lugar - VERB\n",
      "? - PUNCT\n"
     ]
    }
   ],
   "source": [
    "nlp_es = spacy.load('es')\n",
    "\n",
    "doc = nlp(\"Qué onda amiguitoooooo, cómo está todo en ese bello lugar?\")\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "Often, we need to label a span of text; that is, a contiguous multitoken boundary. For example, consider the sentence, “Mary slapped the green witch.” We might want to identify the noun phrases (NP) and verb phrases (VP) in it, as shown here:\n",
    "\n",
    "```\n",
    "[NP Mary] [VP slapped] [the green witch].\n",
    "```\n",
    "\n",
    "This is called chunking or shallow parsing. Shallow parsing aims to derive higher-order units composed of the grammatical atoms, like nouns, verbs, adjectives, and so on. It is possible to write regular expressions over the part-of-speech tags to approximate shallow parsing if you do not have data to train models for shallow parsing. Fortunately, for English and most extensively spoken languages, such data and pretrained models exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary - NP\n",
      "the green witch - NP\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc  = nlp(u\"Mary slapped the green witch.\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print ('{} - {}'.format(chunk, chunk.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary       | nsubj  --> slapped\n",
      "slapped    | ROOT   --> slapped\n",
      "the        | det    --> witch\n",
      "green      | amod   --> witch\n",
      "witch      | dobj   --> slapped\n",
      ".          | punct  --> slapped\n"
     ]
    }
   ],
   "source": [
    "for t in doc:\n",
    "    print(\"{:<10} | {:<6} --> {}\".format(str(t), t.dep_, t.head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No         | ADV    | advmod <- teníamos\n",
      "teníamos   | VERB   | ROOT   <- teníamos\n",
      "nada       | PRON   | obj    <- teníamos\n",
      "para       | ADP    | mark   <- perder\n",
      "perder     | VERB   | acl    <- nada\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "nlp = spacy.load('es')\n",
    "doc  = nlp(u\"No teníamos nada para perder\")\n",
    "for t in doc:\n",
    "    print(\"{:<10} | {:<6} | {:<6} <- {}\".format(str(t), t.pos_, t.dep_, t.head))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es sacado de Twitter... no entiendo qué tan bien está!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Señor      | NOUN   | nsubj  <- chífleme\n",
      "FBI        | PROPN  | flat   <- Señor\n",
      ",          | PUNCT  | punct  <- viendo\n",
      "si         | SCONJ  | mark   <- viendo\n",
      "está       | AUX    | aux    <- viendo\n",
      "viendo     | VERB   | acl    <- Señor\n",
      "esto       | PRON   | obj    <- viendo\n",
      ":          | PUNCT  | punct  <- chífleme\n",
      "chífleme   | VERB   | ROOT   <- chífleme\n",
      "por        | ADP    | case   <- favor\n",
      "favor      | NOUN   | obl    <- chífleme\n",
      "si         | SCONJ  | mark   <- tiene\n",
      "tiene      | VERB   | advcl  <- chífleme\n",
      "un         | DET    | det    <- laburito\n",
      "laburito   | NOUN   | obj    <- tiene\n",
      "en         | ADP    | case   <- blanco\n",
      "blanco     | NOUN   | nmod   <- laburito\n",
      "con        | ADP    | case   <- aguinaldo\n",
      "aguinaldo  | NOUN   | nmod   <- laburito\n",
      "por        | ADP    | case   <- ahí\n",
      "ahí        | ADV    | advmod <- aguinaldo\n"
     ]
    }
   ],
   "source": [
    "doc  = nlp(u\"Señor FBI, si está viendo esto: chífleme por favor si tiene un laburito en blanco con aguinaldo por ahí\")\n",
    "for t in doc:\n",
    "    print(\"{:<10} | {:<6} | {:<6} <- {}\".format(str(t), t.pos_, t.dep_, t.head))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
